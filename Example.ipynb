{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# webSpyder\n",
    "A little class for making the spyder job easier, it's in a really early point of development but it should work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example:\n",
    "A spyder that search wikipedia for page about food"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T10:45:49.563256Z",
     "start_time": "2018-03-13T10:45:45.593660Z"
    }
   },
   "outputs": [],
   "source": [
    "import webSpyder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an istance of the spyder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T10:45:49.574266Z",
     "start_time": "2018-03-13T10:45:49.565260Z"
    }
   },
   "outputs": [],
   "source": [
    "s = webSpyder.Spyder(\"wikipediaSpyder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the url of the page from which the spyder will start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T10:45:49.604788Z",
     "start_time": "2018-03-13T10:45:49.582772Z"
    }
   },
   "outputs": [],
   "source": [
    "s.set_start_url(\"https://it.wikipedia.org/wiki/Pagina_principale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your custom function which decide if the current url have to be parsed or not.\n",
    "\n",
    "You can add as many function as you wish.\n",
    "\n",
    "The Spyder take the and of all the filter functions, so all of them have to return True so that the url is parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T10:45:49.624306Z",
     "start_time": "2018-03-13T10:45:49.609292Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_filter(url):\n",
    "    return \"wikipedia\" in url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T10:45:49.649819Z",
     "start_time": "2018-03-13T10:45:49.633308Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s.set_filter(my_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your custom function which recive the soup (bs4 parse html) and the url of the current page.\n",
    "\n",
    "You can add as many function as you wish.\n",
    "\n",
    "Each function is called on every page the spyder decide to download and parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T10:45:49.660327Z",
     "start_time": "2018-03-13T10:45:49.652322Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_function(soup,url):\n",
    "    print(\"hi the url is %s\\nand the html is %s\"%(url,soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T10:45:49.677339Z",
     "start_time": "2018-03-13T10:45:49.667333Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s.set_function(test_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T15:20:13.928604Z",
     "start_time": "2018-02-27T15:20:13.923599Z"
    }
   },
   "source": [
    "The current state of the spyder is printable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T10:45:49.714365Z",
     "start_time": "2018-03-13T10:45:49.682345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Status:\n",
      "settings:\n",
      "{\n",
      "    \"data_type\": \"list\",\n",
      "    \"mode\": \"wget\",\n",
      "    \"permessive_exception\": true,\n",
      "    \"start_url\": \"https://it.wikipedia.org/wiki/Pagina_principale\",\n",
      "    \"project\": \"wikipediaSpyder\",\n",
      "    \"clear_html\": false,\n",
      "    \"clear_comments\": true,\n",
      "    \"useless_tags\": [\n",
      "        \"svg\",\n",
      "        \"input\",\n",
      "        \"noscript\",\n",
      "        \"link\",\n",
      "        \"script\",\n",
      "        \"style\",\n",
      "        \"iframe\",\n",
      "        \"canvas\"\n",
      "    ],\n",
      "    \"useless_attributes\": [\n",
      "        \"style\",\n",
      "        \"href\",\n",
      "        \"role\",\n",
      "        \"src\",\n",
      "        \"target\",\n",
      "        \"type\",\n",
      "        \"lang\",\n",
      "        \"async\",\n",
      "        \"crossorigin\"\n",
      "    ],\n",
      "    \"skip_estensions\": true,\n",
      "    \"not_skip_estensions_list\": [\n",
      "        \"html\",\n",
      "        \"htm\",\n",
      "        \"php\",\n",
      "        \"aspx\",\n",
      "        \"asp\",\n",
      "        \"axd\",\n",
      "        \"asx\",\n",
      "        \"asmx\",\n",
      "        \"ashx\",\n",
      "        \"cfm\",\n",
      "        \"xml\",\n",
      "        \"rss\",\n",
      "        \"cgi\",\n",
      "        \"jsp\",\n",
      "        \"jspx\"\n",
      "    ],\n",
      "    \"cache\": false,\n",
      "    \"cache_path\": \"C:\\\\Users\\\\zommiommy\\\\Documents\\\\GitHub\\\\webSpyder\\\\webSpyder/pagecaches/\",\n",
      "    \"log\": true,\n",
      "    \"log_path\": \"C:\\\\Users\\\\zommiommy\\\\Documents\\\\GitHub\\\\webSpyder\\\\webSpyder/log/\",\n",
      "    \"log_format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
      "}\n",
      "urls:\n",
      "['https://it.wikipedia.org/wiki/Pagina_principale']\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can do a single url at time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T10:45:49.959038Z",
     "start_time": "2018-03-13T10:45:49.718869Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FileNotFoundError' object has no attribute 'message'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Documents\\GitHub\\webSpyder\\webSpyder\\spyder.py\u001b[0m in \u001b[0;36mpermissive_check_and_parse\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_and_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\webSpyder\\webSpyder\\spyder.py\u001b[0m in \u001b[0;36mcheck_and_parse\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_url_filer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m             \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\webSpyder\\webSpyder\\urls_function.py\u001b[0m in \u001b[0;36mget_page\u001b[1;34m(url, settings, logger)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"wget\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mget_page_methods\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwget_get_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\webSpyder\\webSpyder\\get_page_methods\\wget.py\u001b[0m in \u001b[0;36mwget_get_page\u001b[1;34m(url, name, directory, logger)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# Wait for the wget to finish\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[1;32mwhile\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Impossibile trovare il percorso specificato: 'C:\\\\Users\\\\zommiommy\\\\Documents\\\\GitHub\\\\webSpyder\\\\webSpyder/pagecaches/'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-143c7824fcbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\webSpyder\\webSpyder\\spyder.py\u001b[0m in \u001b[0;36miteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"permessive_exception\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermissive_check_and_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_and_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\webSpyder\\webSpyder\\spyder.py\u001b[0m in \u001b[0;36mpermissive_check_and_parse\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ERROR At the iteration over %s\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FileNotFoundError' object has no attribute 'message'"
     ]
    }
   ],
   "source": [
    "s.iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or it can go on parsing urls that it find in the pages until there aren't anyone left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T10:45:49.960539Z",
     "start_time": "2018-03-13T10:45:45.814Z"
    }
   },
   "outputs": [],
   "source": [
    "s.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be stopped throwing a KeyboardInerrupt pressing Ctrl-C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T10:45:49.964542Z",
     "start_time": "2018-03-13T10:45:45.861Z"
    }
   },
   "outputs": [],
   "source": [
    "import webSpyder\n",
    "\n",
    "s = webSpyder.Spyder(\"wikipediaSpyder\")\n",
    "\n",
    "s.set_start_url(\"https://it.wikipedia.org/\")\n",
    "\n",
    "def my_filter(url):\n",
    "    return \"wikipedia\" in url\n",
    "s.set_filter(my_filter)\n",
    "\n",
    "def test_function(soup,url):\n",
    "    print(\"hi the url is %s\\nand the html is %s\"%(url,soup))\n",
    "s.set_function(test_function)\n",
    "\n",
    "s.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `mode`: choose the wey the page will be downloaded, the default way is by wget with the value \"wget\". For now there is only wget but in the future it will support [\"wget\",\"urllib\",\"selenium\",\"requests\"] librarys.\n",
    "* `permessive_exception`: if permessive_exception is True then if there is an exception in the parsing of the page the spyder will just ignore that page. Else if it is false the spyder will throw the exception.\n",
    "* `start_url`: is the url from which the spyder will start\n",
    "* `project`: name of the spyder, it's the name of the default logfile and it can be usefull in mulithreading situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  `enable_log()`: enable the logging\n",
    "*  `disable_log()`: disable the logging\n",
    "*  `set_log_path(path)`: sets the absolute path of the log file ex. \"C:\\log.log\"\n",
    "*  `set_log_format(\"format\")`: sets the format of the log (same syntax of the logging module format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  `enable_cache()`: enable the caching of the pages\n",
    "*  `disable_cache()`: disable the caching of the pages\n",
    "*  `set_cache_path(path)`: set the absolute path to the folder where the pages files will be cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML Clean functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to eliminate unwanted part of the html like comments or script tags\n",
    "\n",
    "Clear html is the main switch , it enable or disable all the other functions\n",
    "*  `enable_clear_html()`: enable the clear html function\n",
    "*  `disable_clear_html()`: disable the clear html function\n",
    "*  `enable_clear_comments()`: the spyder will delete the comments from the html\n",
    "*  `disable_clear_comments()`:  the spyder will NOT delete the comments from the html\n",
    "*  `set_useless_attributes(attributes_list)`:  set the list of attributes that will be eliminated (if there are any) from the html in every tags. ex. set_useless_attributes([\"style\",\"id\"]) will eliminate every style or id attribute from the tags.\n",
    "*  `set_useless_tags(tags_list)`: set the list of tags that will be eliminated (if there are any) from the html. ex.set_useless_tags([\"img\",\"p\"]) will eliminate every img and p tag from the html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T16:15:26.372442Z",
     "start_time": "2018-02-27T16:15:26.368941Z"
    },
    "collapsed": true
   },
   "source": [
    "## Search Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search work somewhat like disktra's algorithm where instead of having the cost on the edges of the graph it has them it on the nodes and it get the minimum spanning tree of the page link's graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `set_cost_function(f)` the function f(soup,url) has to return the cost of the page which is a real non negative number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default cost function return always 1 so that the spyder will do a width first search.\n",
    "The more the max cost is bigger then the min cost, decided by the function, the more the algorithm will tend to do a depth first search."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
